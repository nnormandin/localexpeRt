---
title: "Understanding localexpeRt"
author: "Nick Normandin"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Method

The method implemented in this package is for use in machine learning tasks involving the prediction of a continuous variable. While there are many choices available for regression algorithms, Binary Local Expert Regression is novel in a few ways:

* it's an ensemble, meaning the predictions of many models are combined
* the concept of stacked regression, or "stacking" is used to train multiple layers of models (ie: meta-feature learning)
* the continuous target variable is transformed into many discrete (specifically binary) target variables, decomposing the original problem into many simpler problems
* local expert predictions are reconstructed to form a unique predicted target variable distribution for each instance

## Data

In this example I will use Quantitative Structure-Activity Relationship (QSAR) data from Max Kuhn's `AppliedPredictiveModeling` package. QSAR modeling is used to predict the properties of chemical compounds (eg. to determine feasibility of a compound's medicinal use).

```{r message = FALSE}
require(AppliedPredictiveModeling)
require(localexpeRt)
## TODO: add gbm package dependency

# bring data into environment
data(solubility)

# rename for simplicity
x <- solTrainXtrans
y <- solTrainY
```

The data set consists of 951 instances, each with 228 attributes. You can verify this using:

```{r}
dim(x)
```

You can use `glimpse(x)` to see that the attributes are a mix of continuous and binary. Some examples of continuous attributes are the molecular weight of the compound (`MolWeight`), number of atoms (`NumAtoms`), and the number of aromatic bonds (`NumAromaticBonds`). There are 208 fairly sparse binary attributes labeled `FP001` to `FP208`.

We can quickly verify that there are no missing or `NA` values:

```{r}
sum(sapply(x, function(x) sum(is.na(x))))
```

The target variable we are trying to predict is the solubility of a compound. Let's take a look at how it is distributed in our 951 instances.

```{r, fig.width=6}
hist(y, main = '', xlab = 'Solubility', breaks = 40)
```

## Training a traditional regression model

Before going into the specifics of the `localexpeRt` method, we will demonstrate how to solve this type of problem in R using traditional regression methods with Max Kuhn's excellent `caret` package. This package is used as a wrapper for a number of CRAN libraries to standardize the pre-processing, training, and testing procedures common in data science tasks. You can learn more about using `caret` at \url{topepo.github.io/caret/}.

The `train` function is used to fit models to data. We'll specify a linear model (ordinary least squares) and leave the `trControl` argument blank for now (this will default to using 25 bootstrap samples).

```{r, warning=FALSE}
require(caret, quietly = TRUE)
linear.model <- train(x, y, method = 'lm')
class(linear.model) # determine object type
linear.model$results$Rsquared # check model performance
```

Note that this is roughly equivalent to the very common `lm(y ~ x)`, but allows us to use a standard format and leverage the powerful pre-processing and testing tools available to `caret`. You can explore the remarkable amount of information stored in the `train` object by executing `str(linear.model)`.


## Training local experts

Applying the `localexpeRt` method to this data set involves considerably more steps. The added accuracy and distributional understanding may not be worth the effort (not to mention the computational cost) in some cases. For example, if:

  * A simple model provides a good fit already
  * Marginally improving model accuracy does not provide a signficant benefit
  * Developing a deeper understanding of prediction variance is unimportant

In this case, however, we will assume (due to the extraordinary cost of pharmaceutical research) that even a slight increase in predictive accuracy is worth a few extra keystrokes.

#### Determine break points

The first step in using the local expert method to solve a regression problem is to encode the continuous target variable vector as a binary matrix. Each column in this matrix represents an additional local expert to be trained and included in the ensemble. Including more models/columns increases the granularity of the method. Having too few models will result in poor prediction accuracy, but improvement is not linear with each model addition and eventually there is a negative marginal benefit. While the number of models to include varies between domains, it's safe to use 20-30 in most applications.

We will accomplish the binary encoding using indicator functions with criteria values placed throughout the expected range of the target variable. The two ways of determing the location of these 'break points' within the expected response range are:

  1. Equally Wide (EW) Intervals: break points are placed at equidistant values throughout the expected range of the target. The distance between each break point is calculated using the user-defined total number of models/columns desired. This form of interval designation is equivalent to the bins created when plotting histograms.
  
  2. Equally Probable (EP) Intervals: break points are placed at equally wide _quantiles_ of the target. The advantage to this method is that an equal number of instances is between each break point, making it easy to understand the extent of class imbalance in each separate modeling task. A disadvantage to this method is that it may produce less reliable estimates of outliers if there are long tails in the target variable distribution.
  
The decision of `n` (the number of columns/models to use) and EP vs EW binning may require experimentation. Results (in terms of final model regression accuracy) differ depending on the local expert model family, meta-features included, and regression stacking model family. In this example, we will use 30 EP break points


```{r, eval = FALSE} 
# convert the response into binary columns
# use equally probable, output 20 columns
y.cols <- BinCols(y, n = 30, mode = 'EW')
```



```{r, eval = FALSE}
# examine breakpoints over histogram of response
hist(y, main = '', xlab = 'Solubility')
abline(v = y.cols$y.vals)
```


#### Train local experts

```{r, eval = FALSE}
# train Local Experts on each of the binary columns
# use default train control object with Support Vector Machines algorithm and linear kernel
model.list <- TrainLEs(x, y.cols$cols, method = 'lda')

# examine ensemble performance by extracting all model info
model.info <- ExtractModelInfo(model.list)

# plot the model information
PlotLEs(model.info)

```

#### Smooth prediction distributions

```{r, eval = FALSE}
# examine the fit of a random instance
instance.id <- sample(seq(1, nrow(model.info$preds.matrix)),1)
instance.check <- FitInstance(model.info$preds.matrix[instance.id,],
                              y.values = y.cols$y.vals, plot = TRUE)

# fit every instance
LE.fits <- FitMatrix(model.info$preds.matrix, y.cols$y.vals)
```



## Building a stacked regression model

#### Create stacking input data frame

```{r, eval = FALSE}
# create meta feature input matrix for stacked regression model
meta.x <- as.data.frame(cbind(model.info$preds.matrix, LE.fits$mean))
meta.x <- do.call(cbind, lapply(meta.x, as.numeric))
cnames <- c(paste0('c', seq(1, ncol(meta.x)-1)), 'mean')
colnames(meta.x) <- cnames
```

#### Build a stacked model

```{r, eval = FALSE}
# test a regression model
# create train control object
trControl <- trainControl(method = "cv", number = 10,
                          returnData = TRUE,
                          savePredictions = TRUE)

# create stacked model
L1.model <- train(x = meta.x, y = y, method = 'gbm', trControl = trControl)
```


## Using the ensemble


#### The long way

```{r, eval = FALSE}

# receive new data, pick a random row to predict
x.new <- solTestXtrans[sample(seq(solTestXtrans),1),]

# pass through local expert ensemble
x.new.LEs <- PredictLEs(x.new, model.list)

# fit the local expert CDF
x.new.fit <- FitInstance(x.new.LEs, y.cols$y.vals, plot = TRUE)

# append mean to x.new.LEs
x1 <- c(x.new.LEs, x.new.fit$mean)
names(x1)[length(x1)] <- 'mean'

y.new <- predict(L1.model, newdata = t(x1))
```


#### The short way

```{r, eval = FALSE}

# receive new data, pick a random row to predict
x.new <- solTestXtrans[sample(seq(solTestXtrans),1),]

y.new <- PredictNew(x = x.new, LE.model.list = model.list, stack.model = L1.model,
                    y.vals = y.cols$y.vals, plot = TRUE, cred = 0.5)

```






















